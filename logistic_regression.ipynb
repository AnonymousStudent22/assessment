{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules for preprocessing \n",
    "import os\n",
    "import skimage\n",
    "from skimage import data\n",
    "from skimage import io\n",
    "from skimage.io import imread, imshow\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "import natsort\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "\n",
    "### Function image_to_vec() will:\n",
    "####  - Read in the image file names in the directory and sort them in ascending order.\n",
    "####  - Add the directory path to each image name, so all images can be found.\n",
    "#### - Find the images using the paths.\n",
    "#### - Read in the images as grayscale\n",
    "#### - Convert the images into arrays\n",
    "#### - Return a list of image arrays and a list of each file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagefile_list = os.listdir('dataset/sample_images/') # dataset of 200\n",
    "directory = 'dataset/sample_images/'\n",
    "\n",
    "#print(os.getcwd()) # check current directory\n",
    "#imagefile_list = os.listdir('test/image/') # test dataset of 200\n",
    "#directory = 'test/image/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to take image files in dir, convert them into vectors and save them to a list\n",
    "\n",
    "def image_to_vec(file_list, directory):    \n",
    "    vec_list = []\n",
    "    files = []\n",
    "    for filename in file_list:\n",
    "        if filename.endswith('.jpg'):\n",
    "            files.append(filename)\n",
    "    files = natsort.natsorted(files) # sort file names in ascending order\n",
    "    #print(files)\n",
    "    for file in files:\n",
    "      image_path = directory+file\n",
    "      #print(image_path)\n",
    "      image = np.array(imread(image_path, as_gray=True)) #read in as grayscale\n",
    "      vec_list.append(image)\n",
    "\n",
    "    return vec_list,files \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 1. Read in the binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the csv file\n",
    "tumors = pd.read_csv('./dataset/sample_labels_0_1.csv', sep=',')\n",
    "#print(tumors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Split the file names and label data into their own lists. Run the file names and labels through the function that will find the images in the directory, and convert them into an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data\n",
    "im_files = list(tumors['file_name']) # converting to list in order to access each filename\n",
    "labels = tumors['label']\n",
    "#print(labels)\n",
    "\n",
    "im_vecs,filenames = image_to_vec(im_files,directory) \n",
    "\n",
    "# plt.imshow(im_vecs[0], cmap='gray') #check that im_vecs has image vectors in correct order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Merge the image and label data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]), 1)\n"
     ]
    }
   ],
   "source": [
    "data = list(zip(im_vecs,labels)) # join the image vectors and their respective labels\n",
    "data # output should show that each array is paired with its label\n",
    "print(data[0])\n",
    "#print('data type',type(data))\n",
    "#for i in range(len(data)):\n",
    "    #print('index:',i,'label:',labels[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Convert the image and label data into arrays X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data file: \n",
    "\n",
    "def list_to_array(list_of_tuples): # Convert the data from list type to array\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    count=0\n",
    "    for pair in list_of_tuples:\n",
    "        temp=pair[0]\n",
    "        temp2=pair[1]\n",
    "        X.append(temp)\n",
    "        Y.append(temp2)\n",
    "        count+=1\n",
    "        #print(count,'tuples converted to array') #comment out to see check function is running\n",
    "    X=np.asarray(X)\n",
    "    Y=np.asarray(Y)\n",
    "    \n",
    "    return X,Y\n",
    "\n",
    "X,Y=list_to_array(data)\n",
    "X=X.reshape(200,262144) # MAKE SURE THIS MATCHES COUNT OF DATA SAMPLES\n",
    "\n",
    "\n",
    "# Shuffle\n",
    "X, Y = shuffle(X,Y)\n",
    "#print('x shape',X.shape, 'y shape',Y.shape)\n",
    "\n",
    "#Split the data into training and test (validation) set, if there's no real test data \n",
    "##x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7, random_state=0)\n",
    "\n",
    "#print('x_train shape',x_train.shape,'y_train shape',y_train.shape)\n",
    "#print('x_test shape',x_test.shape,'y_test shape',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape (200, 262144) Y_test.shape (200,)\n",
      "X.shape (200, 262144) Y.shape (200,)\n"
     ]
    }
   ],
   "source": [
    "imagefile_list_test = os.listdir('test/image/') # test dataset of 200\n",
    "directory_test = 'test/image/'\n",
    "tumors_test = pd.read_csv('test/labels_0_1.csv', sep=',') # binary class labels\n",
    "#print(tumors_test)\n",
    "\n",
    "\n",
    "# Split the data\n",
    "im_files_test = list(tumors_test['file_name']) # converting to list in order to access each filename\n",
    "labels_test = tumors_test['label2']\n",
    "#print(labels_test)\n",
    "\n",
    "\n",
    "\n",
    "# Convert image tpo arpray\n",
    "im_vecs_test,filenames_test = image_to_vec(im_files_test,directory_test) \n",
    "\n",
    "\n",
    "\n",
    "#plt.imshow(im_vecs_test[0], cmap='gray') #check that im_vecs has image vectors in correct order\n",
    "\n",
    "# Join image vectors with labels\n",
    "data_test = list(zip(im_vecs_test,labels_test))\n",
    "\n",
    "\n",
    "# Convert list of arrays into array of arrays\n",
    "X_test,Y_test=list_to_array(data_test)\n",
    "X_test=X_test.reshape(200,262144) # MAKE SURE THIS MATCHES COUNT OF DATA SAMPLES\n",
    "\n",
    "print('X_test.shape',X_test.shape,'Y_test.shape',Y_test.shape)\n",
    "print('X.shape',X.shape,'Y.shape',Y.shape)\n",
    "\n",
    "\n",
    "# extracting training and validation data from training dataset \n",
    "x_train, x_val, y_train, y_val = train_test_split(X, Y, train_size=0.7, random_state=0)\n",
    "\n",
    "X_test,Y_test = shuffle(X_test,Y_test)\n",
    "# extracting test data from test dataset and discarding training portion by asssigning\n",
    "# it to dummy variables that won't be used.\n",
    "x_dummy, x_test, y_dummy, y_test = train_test_split(X_test, Y_test, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1. / (1. + np.exp(-z))  # z=feature vector xTrain times parameter vector theta  \n",
    "\n",
    "\n",
    "def param_update(xTrain, yTrain):\n",
    "    #print('xTrain shape before',xTrain.shape) \n",
    "    new_col = np.zeros((xTrain.shape[0], 1)) \n",
    "    xTrain = np.append(xTrain, new_col, axis=1)\n",
    "    #print('xTrain shape after',xTrain.shape)\n",
    "    \n",
    "    #initialise parameters\n",
    "    theta = np.zeros(xTrain.shape[1]) \n",
    "    epoch=1440\n",
    "    alpha = 0.01 \n",
    "   \n",
    "    for i in range(epoch): # instead of epoch,loop until close to zero\n",
    "        z = np.dot(xTrain, theta) # feature vector times parameter vector\n",
    "        h = sigmoid(z)  \n",
    "        gradient = theta - alpha * np.dot(xTrain.T,(h-yTrain))/yTrain.shape[0]\n",
    "        theta = gradient \n",
    "        #print out shows at which iteration the function starts to converge,\n",
    "        # meaning that the values change more slowly\n",
    "        print('sum(gradient)',i,sum(gradient))\n",
    "  \n",
    "    return theta\n",
    "\n",
    "def train_predict(xTrain, yTrain, xVal,yVal):\n",
    "    xTrain = xTrain - xTrain.mean() # normalise\n",
    "    #print('xVal shape before',xVal.shape) \n",
    "    theta = param_update(xTrain, yTrain)\n",
    "    new_col = np.ones((xVal.shape[0], 1)) \n",
    "    xVal = np.append(xVal, new_col, axis=1)\n",
    "    #print('xVal shape after',xVal.shape)\n",
    "    z = np.dot(xVal,theta)\n",
    "    h = sigmoid(z)\n",
    "    y_pred = h >= 0.5 # true or false assignment \n",
    "    score = accuracy_score(yVal,y_pred)\n",
    "    \n",
    "    return y_pred, score, theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test the model on training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum(gradient) 0 -11.368231732693284\n",
      "sum(gradient) 1 -21.07709733334238\n",
      "sum(gradient) 2 -22.416102447011472\n",
      "sum(gradient) 3 -1.28191305681831\n",
      "sum(gradient) 4 -14.32175190115663\n",
      "sum(gradient) 5 0.32494816748699396\n",
      "sum(gradient) 6 -12.672270827989587\n",
      "sum(gradient) 7 -6.472944028876918\n",
      "sum(gradient) 8 -17.49448331300574\n",
      "sum(gradient) 9 -8.906867333008643\n",
      "sum(gradient) 10 -11.923926283812149\n",
      "sum(gradient) 11 -12.039367881249376\n",
      "sum(gradient) 12 -12.408338782180028\n",
      "sum(gradient) 13 -12.79023474490435\n",
      "sum(gradient) 14 -13.085044106816193\n",
      "sum(gradient) 15 -13.440151626295576\n",
      "sum(gradient) 16 -13.703850647806108\n",
      "sum(gradient) 17 -14.02542327712893\n",
      "sum(gradient) 18 -14.272104622464452\n",
      "sum(gradient) 19 -14.55794831083109\n",
      "sum(gradient) 20 -14.794628444279383\n",
      "sum(gradient) 21 -15.048616133800742\n",
      "sum(gradient) 22 -15.275484490186438\n",
      "sum(gradient) 23 -15.50471964275166\n",
      "sum(gradient) 24 -15.719708063371023\n",
      "sum(gradient) 25 -15.930457128288813\n",
      "sum(gradient) 26 -16.132575484812925\n",
      "sum(gradient) 27 -16.328797376403\n",
      "sum(gradient) 28 -16.5185116675188\n",
      "sum(gradient) 29 -16.702485102321777\n",
      "sum(gradient) 30 -16.88090181026879\n",
      "sum(gradient) 31 -17.054118307749913\n",
      "sum(gradient) 32 -17.22240188201619\n",
      "sum(gradient) 33 -17.386017737542076\n",
      "sum(gradient) 34 -17.54520630750989\n",
      "sum(gradient) 35 -17.700191625270268\n",
      "sum(gradient) 36 -17.851181408301446\n",
      "sum(gradient) 37 -17.99836884059186\n",
      "sum(gradient) 38 -18.141933809976372\n",
      "sum(gradient) 39 -18.28204407903576\n",
      "sum(gradient) 40 -18.41885632374168\n",
      "sum(gradient) 41 -18.55251706714445\n",
      "sum(gradient) 42 -18.68316351882782\n",
      "sum(gradient) 43 -18.8109243312186\n",
      "sum(gradient) 44 -18.93592028220691\n",
      "sum(gradient) 45 -19.058264892331554\n",
      "sum(gradient) 46 -19.178064983788225\n",
      "sum(gradient) 47 -19.29542118757917\n",
      "sum(gradient) 48 -19.410428404383108\n",
      "sum(gradient) 49 -19.5231762240215\n",
      "sum(gradient) 50 -19.63374930784623\n",
      "sum(gradient) 51 -19.742227737847365\n",
      "sum(gradient) 52 -19.848687335844577\n",
      "sum(gradient) 53 -19.953199955738274\n",
      "sum(gradient) 54 -20.05583375148006\n",
      "sum(gradient) 55 -20.156653423095477\n",
      "sum(gradient) 56 -20.25572044285839\n",
      "sum(gradient) 57 -20.353093263496703\n",
      "sum(gradient) 58 -20.44882751006773\n",
      "sum(gradient) 59 -20.542976157029663\n",
      "sum(gradient) 60 -20.635589691821927\n",
      "sum(gradient) 61 -20.726716266160125\n",
      "sum(gradient) 62 -20.816401836133505\n",
      "sum(gradient) 63 -20.904690292060184\n",
      "sum(gradient) 64 -20.991623578992\n",
      "sum(gradient) 65 -21.077241808643503\n",
      "sum(gradient) 66 -21.16158336347279\n",
      "sum(gradient) 67 -21.24468499354381\n",
      "sum(gradient) 68 -21.326581906776678\n",
      "sum(gradient) 69 -21.40730785309645\n",
      "sum(gradient) 70 -21.486895202981856\n",
      "sum(gradient) 71 -21.565375020832043\n",
      "sum(gradient) 72 -21.64277713357307\n",
      "sum(gradient) 73 -21.719130194853076\n",
      "sum(gradient) 74 -21.794461745165393\n",
      "sum(gradient) 75 -21.86879826820012\n",
      "sum(gradient) 76 -21.94216524370336\n",
      "sum(gradient) 77 -22.01458719710626\n",
      "sum(gradient) 78 -22.086087746138315\n",
      "sum(gradient) 79 -22.156689644662084\n",
      "sum(gradient) 80 -22.22641482390489\n",
      "sum(gradient) 81 -22.29528443129018\n",
      "sum(gradient) 82 -22.363318867002757\n",
      "sum(gradient) 83 -22.43053781848534\n",
      "sum(gradient) 84 -22.496960292962576\n",
      "sum(gradient) 85 -22.56260464815324\n",
      "sum(gradient) 86 -22.627488621281206\n",
      "sum(gradient) 87 -22.69162935649143\n",
      "sum(gradient) 88 -22.755043430779114\n",
      "sum(gradient) 89 -22.817746878535335\n",
      "sum(gradient) 90 -22.879755214774054\n",
      "sum(gradient) 91 -22.941083457149855\n",
      "sum(gradient) 92 -23.00174614682091\n",
      "sum(gradient) 93 -23.06175736823931\n",
      "sum(gradient) 94 -23.12113076793429\n",
      "sum(gradient) 95 -23.179879572337146\n",
      "sum(gradient) 96 -23.238016604727836\n",
      "sum(gradient) 97 -23.29555430132128\n",
      "sum(gradient) 98 -23.352504726586943\n",
      "sum(gradient) 99 -23.408879587798722\n",
      "sum(gradient) 100 -23.46469024890762\n",
      "sum(gradient) 101 -23.519947743732388\n",
      "sum(gradient) 102 -23.57466278853744\n",
      "sum(gradient) 103 -23.628845794013863\n",
      "sum(gradient) 104 -23.68250687671202\n",
      "sum(gradient) 105 -23.735655869940405\n",
      "sum(gradient) 106 -23.78830233416694\n",
      "sum(gradient) 107 -23.840455566957043\n",
      "sum(gradient) 108 -23.8921246124597\n",
      "sum(gradient) 109 -23.94331827046872\n",
      "sum(gradient) 110 -23.994045105097026\n",
      "sum(gradient) 111 -24.044313453055704\n",
      "sum(gradient) 112 -24.09413143158898\n",
      "sum(gradient) 113 -24.143506946060164\n",
      "sum(gradient) 114 -24.19244769720904\n",
      "sum(gradient) 115 -24.240961188120394\n",
      "sum(gradient) 116 -24.28905473087783\n",
      "sum(gradient) 117 -24.336735452960305\n",
      "sum(gradient) 118 -24.384010303363663\n",
      "sum(gradient) 119 -24.430886058472904\n",
      "sum(gradient) 120 -24.4773693277001\n",
      "sum(gradient) 121 -24.52346655889283\n",
      "sum(gradient) 122 -24.5691840435233\n",
      "sum(gradient) 123 -24.614527921680587\n",
      "sum(gradient) 124 -24.659504186852317\n",
      "sum(gradient) 125 -24.704118690535605\n",
      "sum(gradient) 126 -24.74837714665793\n",
      "sum(gradient) 127 -24.792285135831925\n",
      "sum(gradient) 128 -24.835848109446527\n",
      "sum(gradient) 129 -24.879071393607273\n",
      "sum(gradient) 130 -24.92196019291865\n",
      "sum(gradient) 131 -24.964519594137283\n",
      "sum(gradient) 132 -25.006754569682094\n",
      "sum(gradient) 133 -25.048669981014065\n",
      "sum(gradient) 134 -25.09027058190163\n",
      "sum(gradient) 135 -25.13156102155521\n",
      "sum(gradient) 136 -25.17254584765595\n",
      "sum(gradient) 137 -25.213229509280573\n",
      "sum(gradient) 138 -25.253616359708147\n",
      "sum(gradient) 139 -25.293710659143972\n",
      "sum(gradient) 140 -25.333516577334304\n",
      "sum(gradient) 141 -25.37303819610412\n",
      "sum(gradient) 142 -25.41227951179031\n",
      "sum(gradient) 143 -25.451244437605844\n",
      "sum(gradient) 144 -25.489936805915338\n",
      "sum(gradient) 145 -25.5283603704387\n",
      "sum(gradient) 146 -25.56651880837451\n",
      "sum(gradient) 147 -25.604415722456494\n",
      "sum(gradient) 148 -25.642054642942973\n",
      "sum(gradient) 149 -25.679439029535857\n",
      "sum(gradient) 150 -25.716572273241\n",
      "sum(gradient) 151 -25.753457698166198\n",
      "sum(gradient) 152 -25.790098563258763\n",
      "sum(gradient) 153 -25.826498063994237\n",
      "sum(gradient) 154 -25.862659334001766\n",
      "sum(gradient) 155 -25.89858544664248\n",
      "sum(gradient) 156 -25.934279416543443\n",
      "sum(gradient) 157 -25.96974420106994\n",
      "sum(gradient) 158 -26.004982701770242\n",
      "sum(gradient) 159 -26.039997765751327\n",
      "sum(gradient) 160 -26.074792187040025\n",
      "sum(gradient) 161 -26.109368707879216\n",
      "sum(gradient) 162 -26.14373001999937\n",
      "sum(gradient) 163 -26.177878765843044\n",
      "sum(gradient) 164 -26.211817539755486\n",
      "sum(gradient) 165 -26.245548889141297\n",
      "sum(gradient) 166 -26.279075315585093\n",
      "sum(gradient) 167 -26.312399275940198\n",
      "sum(gradient) 168 -26.345523183379303\n",
      "sum(gradient) 169 -26.378449408424817\n",
      "sum(gradient) 170 -26.41118027994247\n",
      "sum(gradient) 171 -26.443718086105132\n",
      "sum(gradient) 172 -26.47606507533565\n",
      "sum(gradient) 173 -26.50822345721383\n",
      "sum(gradient) 174 -26.54019540336489\n",
      "sum(gradient) 175 -26.571983048320014\n",
      "sum(gradient) 176 -26.60358849035375\n",
      "sum(gradient) 177 -26.63501379229489\n",
      "sum(gradient) 178 -26.66626098231958\n",
      "sum(gradient) 179 -26.697332054717773\n",
      "sum(gradient) 180 -26.728228970642846\n",
      "sum(gradient) 181 -26.758953658837125\n",
      "sum(gradient) 182 -26.789508016343493\n",
      "sum(gradient) 183 -26.819893909184156\n",
      "sum(gradient) 184 -26.850113173042587\n",
      "sum(gradient) 185 -26.88016761390619\n",
      "sum(gradient) 186 -26.91005900870638\n",
      "sum(gradient) 187 -26.939789105929666\n",
      "sum(gradient) 188 -26.969359626229117\n",
      "sum(gradient) 189 -26.998772262999655\n",
      "sum(gradient) 190 -27.028028682955025\n",
      "sum(gradient) 191 -27.057130526681384\n",
      "sum(gradient) 192 -27.086079409176627\n",
      "sum(gradient) 193 -27.114876920382674\n",
      "sum(gradient) 194 -27.14352462569171\n",
      "sum(gradient) 195 -27.172024066455677\n",
      "sum(gradient) 196 -27.200376760470533\n",
      "sum(gradient) 197 -27.228584202447294\n",
      "sum(gradient) 198 -27.256647864484062\n",
      "sum(gradient) 199 -27.284569196514372\n",
      "sum(gradient) 200 -27.312349626747302\n",
      "sum(gradient) 201 -27.339990562099363\n",
      "sum(gradient) 202 -27.367493388616435\n",
      "sum(gradient) 203 -27.394859471877485\n",
      "sum(gradient) 204 -27.422090157398777\n",
      "sum(gradient) 205 -27.449186771021836\n",
      "sum(gradient) 206 -27.47615061929326\n",
      "sum(gradient) 207 -27.502982989839367\n",
      "sum(gradient) 208 -27.52968515172418\n",
      "sum(gradient) 209 -27.556258355803024\n",
      "sum(gradient) 210 -27.582703835074224\n",
      "sum(gradient) 211 -27.609022805010007\n",
      "sum(gradient) 212 -27.635216463887698\n",
      "sum(gradient) 213 -27.661285993114852\n",
      "sum(gradient) 214 -27.687232557535097\n",
      "sum(gradient) 215 -27.71305730574632\n",
      "sum(gradient) 216 -27.738761370390147\n",
      "sum(gradient) 217 -27.76434586845236\n",
      "sum(gradient) 218 -27.789811901544958\n",
      "sum(gradient) 219 -27.81516055619094\n",
      "sum(gradient) 220 -27.840392904088535\n",
      "sum(gradient) 221 -27.865510002391595\n",
      "sum(gradient) 222 -27.8905128939563\n",
      "sum(gradient) 223 -27.915402607613036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum(gradient) 224 -27.940180158400274\n",
      "sum(gradient) 225 -27.964846547818784\n",
      "sum(gradient) 226 -27.98940276407141\n",
      "sum(gradient) 227 -28.013849782289324\n",
      "sum(gradient) 228 -28.038188564767665\n",
      "sum(gradient) 229 -28.062420061183392\n",
      "sum(gradient) 230 -28.08654520882245\n",
      "sum(gradient) 231 -28.110564932783678\n",
      "sum(gradient) 232 -28.134480146197326\n",
      "sum(gradient) 233 -28.15829175042297\n",
      "sum(gradient) 234 -28.182000635258\n",
      "sum(gradient) 235 -28.205607679122846\n",
      "sum(gradient) 236 -28.229113749268862\n",
      "sum(gradient) 237 -28.252519701948316\n",
      "sum(gradient) 238 -28.275826382616973\n",
      "sum(gradient) 239 -28.29903462610095\n",
      "sum(gradient) 240 -28.32214525677838\n",
      "sum(gradient) 241 -28.345159088754823\n",
      "sum(gradient) 242 -28.36807692602907\n",
      "sum(gradient) 243 -28.390899562659918\n",
      "sum(gradient) 244 -28.413627782933062\n",
      "sum(gradient) 245 -28.436262361512732\n",
      "sum(gradient) 246 -28.45880406360551\n",
      "sum(gradient) 247 -28.4812536451073\n",
      "sum(gradient) 248 -28.503611852757505\n",
      "sum(gradient) 249 -28.52587942428495\n",
      "sum(gradient) 250 -28.54805708854941\n",
      "sum(gradient) 251 -28.570145565685955\n",
      "sum(gradient) 252 -28.592145567241534\n",
      "sum(gradient) 253 -28.614057796312178\n",
      "sum(gradient) 254 -28.635882947673323\n",
      "sum(gradient) 255 -28.65762170791051\n",
      "sum(gradient) 256 -28.679274755552722\n",
      "sum(gradient) 257 -28.700842761189723\n",
      "sum(gradient) 258 -28.72232638759939\n",
      "sum(gradient) 259 -28.743726289868224\n",
      "sum(gradient) 260 -28.765043115508746\n",
      "sum(gradient) 261 -28.786277504575473\n",
      "sum(gradient) 262 -28.807430089780446\n",
      "sum(gradient) 263 -28.828501496598477\n",
      "sum(gradient) 264 -28.849492343389297\n",
      "sum(gradient) 265 -28.870403241488802\n",
      "sum(gradient) 266 -28.891234795330185\n",
      "sum(gradient) 267 -28.911987602534854\n",
      "sum(gradient) 268 -28.932662254021743\n",
      "sum(gradient) 269 -28.953259334105645\n",
      "sum(gradient) 270 -28.973779420588965\n",
      "sum(gradient) 271 -28.99422308486985\n",
      "sum(gradient) 272 -29.014590892022387\n",
      "sum(gradient) 273 -29.0348834009027\n",
      "sum(gradient) 274 -29.05510116422765\n",
      "sum(gradient) 275 -29.075244728671937\n",
      "sum(gradient) 276 -29.09531463495119\n",
      "sum(gradient) 277 -29.115311417913567\n",
      "sum(gradient) 278 -29.13523560662026\n",
      "sum(gradient) 279 -29.155087724425673\n",
      "sum(gradient) 280 -29.174868289063834\n",
      "sum(gradient) 281 -29.194577812729186\n",
      "sum(gradient) 282 -29.21421680214937\n",
      "sum(gradient) 283 -29.233785758667654\n",
      "sum(gradient) 284 -29.253285178317228\n",
      "sum(gradient) 285 -29.272715551895196\n",
      "sum(gradient) 286 -29.292077365035702\n",
      "sum(gradient) 287 -29.311371098282102\n",
      "sum(gradient) 288 -29.330597227161263\n",
      "sum(gradient) 289 -29.349756222243528\n",
      "sum(gradient) 290 -29.368848549225458\n",
      "sum(gradient) 291 -29.387874668982256\n",
      "sum(gradient) 292 -29.406835037641574\n",
      "sum(gradient) 293 -29.42573010665047\n",
      "sum(gradient) 294 -29.44456032283206\n",
      "sum(gradient) 295 -29.463326128452245\n",
      "sum(gradient) 296 -29.48202796128509\n",
      "sum(gradient) 297 -29.5006662546634\n",
      "sum(gradient) 298 -29.51924143754922\n",
      "sum(gradient) 299 -29.5377539345864\n",
      "sum(gradient) 300 -29.556204166155098\n",
      "sum(gradient) 301 -29.57459254843842\n",
      "sum(gradient) 302 -29.59291949347008\n",
      "sum(gradient) 303 -29.611185409190284\n",
      "sum(gradient) 304 -29.629390699503457\n",
      "sum(gradient) 305 -29.64753576432269\n",
      "sum(gradient) 306 -29.66562099963713\n",
      "sum(gradient) 307 -29.683646797544892\n",
      "sum(gradient) 308 -29.7016135463178\n",
      "sum(gradient) 309 -29.719521630442415\n",
      "sum(gradient) 310 -29.737371430677396\n",
      "sum(gradient) 311 -29.755163324092297\n",
      "sum(gradient) 312 -29.772897684117183\n",
      "sum(gradient) 313 -29.79057488059673\n",
      "sum(gradient) 314 -29.808195279823558\n",
      "sum(gradient) 315 -29.825759244597485\n",
      "sum(gradient) 316 -29.843267134256422\n",
      "sum(gradient) 317 -29.860719304727045\n",
      "sum(gradient) 318 -29.878116108570833\n",
      "sum(gradient) 319 -29.895457895016673\n",
      "sum(gradient) 320 -29.91274501001153\n",
      "sum(gradient) 321 -29.929977796256225\n",
      "sum(gradient) 322 -29.94715659325023\n",
      "sum(gradient) 323 -29.9642817373251\n",
      "sum(gradient) 324 -29.981353561691556\n",
      "sum(gradient) 325 -29.998372396468852\n",
      "sum(gradient) 326 -30.015338568732215\n",
      "sum(gradient) 327 -30.0322524025412\n",
      "sum(gradient) 328 -30.049114218983252\n",
      "sum(gradient) 329 -30.065924336208344\n",
      "sum(gradient) 330 -30.08268306946076\n",
      "sum(gradient) 331 -30.099390731121154\n",
      "sum(gradient) 332 -30.116047630733167\n",
      "sum(gradient) 333 -30.132654075043376\n",
      "sum(gradient) 334 -30.14921036803472\n",
      "sum(gradient) 335 -30.165716810955455\n",
      "sum(gradient) 336 -30.18217370235493\n",
      "sum(gradient) 337 -30.1985813381149\n",
      "sum(gradient) 338 -30.21494001148151\n",
      "sum(gradient) 339 -30.231250013095554\n",
      "sum(gradient) 340 -30.247511631025986\n",
      "sum(gradient) 341 -30.263725150793977\n",
      "sum(gradient) 342 -30.279890855410656\n",
      "sum(gradient) 343 -30.296009025399478\n",
      "sum(gradient) 344 -30.312079938829015\n",
      "sum(gradient) 345 -30.328103871342208\n",
      "sum(gradient) 346 -30.34408109618055\n",
      "sum(gradient) 347 -30.3600118842174\n",
      "sum(gradient) 348 -30.375896503979067\n",
      "sum(gradient) 349 -30.391735221673017\n",
      "sum(gradient) 350 -30.40752830122231\n",
      "sum(gradient) 351 -30.423276004280826\n",
      "sum(gradient) 352 -30.438978590264245\n",
      "sum(gradient) 353 -30.454636316376984\n",
      "sum(gradient) 354 -30.470249437633136\n",
      "sum(gradient) 355 -30.48581820688433\n",
      "sum(gradient) 356 -30.50134287484243\n",
      "sum(gradient) 357 -30.516823690106683\n",
      "sum(gradient) 358 -30.53226089917887\n",
      "sum(gradient) 359 -30.547654746501234\n",
      "sum(gradient) 360 -30.56300547446259\n",
      "sum(gradient) 361 -30.5783133234363\n",
      "sum(gradient) 362 -30.593578531790552\n",
      "sum(gradient) 363 -30.60880133591832\n",
      "sum(gradient) 364 -30.623981970257574\n",
      "sum(gradient) 365 -30.63912066730634\n",
      "sum(gradient) 366 -30.65421765765744\n",
      "sum(gradient) 367 -30.669273170002818\n",
      "sum(gradient) 368 -30.68428743116864\n",
      "sum(gradient) 369 -30.699260666125877\n",
      "sum(gradient) 370 -30.714193098016846\n",
      "sum(gradient) 371 -30.729084948169806\n",
      "sum(gradient) 372 -30.743936436121764\n",
      "sum(gradient) 373 -30.758747779636707\n",
      "sum(gradient) 374 -30.773519194725022\n",
      "sum(gradient) 375 -30.78825089566099\n",
      "sum(gradient) 376 -30.8029430950037\n",
      "sum(gradient) 377 -30.817596003612305\n",
      "sum(gradient) 378 -30.832209830668475\n",
      "sum(gradient) 379 -30.846784783685106\n",
      "sum(gradient) 380 -30.861321068536707\n",
      "sum(gradient) 381 -30.875818889466725\n",
      "sum(gradient) 382 -30.890278449108063\n",
      "sum(gradient) 383 -30.904699948501367\n",
      "sum(gradient) 384 -30.91908358710832\n",
      "sum(gradient) 385 -30.933429562829936\n",
      "sum(gradient) 386 -30.947738072023526\n",
      "sum(gradient) 387 -30.96200930951788\n",
      "sum(gradient) 388 -30.976243468629377\n",
      "sum(gradient) 389 -30.990440741176176\n",
      "sum(gradient) 390 -31.004601317495954\n",
      "sum(gradient) 391 -31.018725386457753\n",
      "sum(gradient) 392 -31.03281313548143\n",
      "sum(gradient) 393 -31.046864750548288\n",
      "sum(gradient) 394 -31.06088041621805\n",
      "sum(gradient) 395 -31.074860315645786\n",
      "sum(gradient) 396 -31.08880463058755\n",
      "sum(gradient) 397 -31.102713541423775\n",
      "sum(gradient) 398 -31.11658722716838\n",
      "sum(gradient) 399 -31.13042586548296\n",
      "sum(gradient) 400 -31.1442296326949\n",
      "sum(gradient) 401 -31.157998703801933\n",
      "sum(gradient) 402 -31.171733252490306\n",
      "sum(gradient) 403 -31.18543345115166\n",
      "sum(gradient) 404 -31.199099470888623\n",
      "sum(gradient) 405 -31.212731481533545\n",
      "sum(gradient) 406 -31.226329651655035\n",
      "sum(gradient) 407 -31.239894148577434\n",
      "sum(gradient) 408 -31.25342513838616\n",
      "sum(gradient) 409 -31.266922785946065\n",
      "sum(gradient) 410 -31.280387254909034\n",
      "sum(gradient) 411 -31.29381870772624\n",
      "sum(gradient) 412 -31.307217305663606\n",
      "sum(gradient) 413 -31.320583208808337\n",
      "sum(gradient) 414 -31.333916576084086\n",
      "sum(gradient) 415 -31.34721756525604\n",
      "sum(gradient) 416 -31.36048633295451\n",
      "sum(gradient) 417 -31.373723034671663\n",
      "sum(gradient) 418 -31.386927824780976\n",
      "sum(gradient) 419 -31.400100856546537\n",
      "sum(gradient) 420 -31.413242282131687\n",
      "sum(gradient) 421 -31.426352252613306\n",
      "sum(gradient) 422 -31.439430917983994\n",
      "sum(gradient) 423 -31.452478427174473\n",
      "sum(gradient) 424 -31.46549492805256\n",
      "sum(gradient) 425 -31.47848056743944\n",
      "sum(gradient) 426 -31.491435491117205\n",
      "sum(gradient) 427 -31.504359843838376\n",
      "sum(gradient) 428 -31.517253769340723\n",
      "sum(gradient) 429 -31.530117410346033\n",
      "sum(gradient) 430 -31.542950908580924\n",
      "sum(gradient) 431 -31.55575440478047\n",
      "sum(gradient) 432 -31.56852803869439\n",
      "sum(gradient) 433 -31.581271949103193\n",
      "sum(gradient) 434 -31.593986273822345\n",
      "sum(gradient) 435 -31.606671149717286\n",
      "sum(gradient) 436 -31.619326712699312\n",
      "sum(gradient) 437 -31.631953097748298\n",
      "sum(gradient) 438 -31.644550438916017\n",
      "sum(gradient) 439 -31.65711886932904\n",
      "sum(gradient) 440 -31.669658521207662\n",
      "sum(gradient) 441 -31.68216952586567\n",
      "sum(gradient) 442 -31.694652013721782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum(gradient) 443 -31.707106114307884\n",
      "sum(gradient) 444 -31.719531956278885\n",
      "sum(gradient) 445 -31.731929667415464\n",
      "sum(gradient) 446 -31.744299374635435\n",
      "sum(gradient) 447 -31.756641204003564\n",
      "sum(gradient) 448 -31.768955280732197\n",
      "sum(gradient) 449 -31.78124172919877\n",
      "sum(gradient) 450 -31.79350067294201\n",
      "sum(gradient) 451 -31.805732234678565\n",
      "sum(gradient) 452 -31.817936536308313\n",
      "sum(gradient) 453 -31.830113698916815\n",
      "sum(gradient) 454 -31.84226384278907\n",
      "sum(gradient) 455 -31.854387087410505\n",
      "sum(gradient) 456 -31.866483551477938\n",
      "sum(gradient) 457 -31.878553352907815\n",
      "sum(gradient) 458 -31.890596608835917\n",
      "sum(gradient) 459 -31.90261343563405\n",
      "sum(gradient) 460 -31.914603948908063\n",
      "sum(gradient) 461 -31.926568263510422\n",
      "sum(gradient) 462 -31.938506493544352\n",
      "sum(gradient) 463 -31.95041875236615\n",
      "sum(gradient) 464 -31.96230515260158\n",
      "sum(gradient) 465 -31.9741658061446\n",
      "sum(gradient) 466 -31.986000824164027\n",
      "sum(gradient) 467 -31.997810317111938\n",
      "sum(gradient) 468 -32.00959439473184\n",
      "sum(gradient) 469 -32.02135316605836\n",
      "sum(gradient) 470 -32.0330867394285\n",
      "sum(gradient) 471 -32.04479522248849\n",
      "sum(gradient) 472 -32.056478722189624\n",
      "sum(gradient) 473 -32.068137344811085\n",
      "sum(gradient) 474 -32.07977119595179\n",
      "sum(gradient) 475 -32.09138038054285\n",
      "sum(gradient) 476 -32.10296500284688\n",
      "sum(gradient) 477 -32.11452516647115\n",
      "sum(gradient) 478 -32.12606097436723\n",
      "sum(gradient) 479 -32.13757252884576\n",
      "sum(gradient) 480 -32.14905993156426\n",
      "sum(gradient) 481 -32.160523283552166\n",
      "sum(gradient) 482 -32.17196268520366\n",
      "sum(gradient) 483 -32.18337823628746\n",
      "sum(gradient) 484 -32.19477003594777\n",
      "sum(gradient) 485 -32.20613818271755\n",
      "sum(gradient) 486 -32.2174827745183\n",
      "sum(gradient) 487 -32.22880390866126\n",
      "sum(gradient) 488 -32.24010168186079\n",
      "sum(gradient) 489 -32.25137619023399\n",
      "sum(gradient) 490 -32.26262752930667\n",
      "sum(gradient) 491 -32.27385579401876\n",
      "sum(gradient) 492 -32.28506107872654\n",
      "sum(gradient) 493 -32.29624347721363\n",
      "sum(gradient) 494 -32.307403082688225\n",
      "sum(gradient) 495 -32.31853998778878\n",
      "sum(gradient) 496 -32.3296542845985\n",
      "sum(gradient) 497 -32.340746064637415\n",
      "sum(gradient) 498 -32.351815418868306\n",
      "sum(gradient) 499 -32.36286243771105\n",
      "sum(gradient) 500 -32.3738872110395\n",
      "sum(gradient) 501 -32.38488982818111\n",
      "sum(gradient) 502 -32.39587037793457\n",
      "sum(gradient) 503 -32.40682894856028\n",
      "sum(gradient) 504 -32.417765627797436\n",
      "sum(gradient) 505 -32.428680502854036\n",
      "sum(gradient) 506 -32.439573660425616\n",
      "sum(gradient) 507 -32.45044518668511\n",
      "sum(gradient) 508 -32.46129516730514\n",
      "sum(gradient) 509 -32.47212368743758\n",
      "sum(gradient) 510 -32.4829308317404\n",
      "sum(gradient) 511 -32.493716684372004\n",
      "sum(gradient) 512 -32.504481328991645\n",
      "sum(gradient) 513 -32.515224848767836\n",
      "sum(gradient) 514 -32.525947326382\n",
      "sum(gradient) 515 -32.53664884403434\n",
      "sum(gradient) 516 -32.54732948344023\n",
      "sum(gradient) 517 -32.55798932584189\n",
      "sum(gradient) 518 -32.56862845200966\n",
      "sum(gradient) 519 -32.57924694224204\n",
      "sum(gradient) 520 -32.589844876372204\n",
      "sum(gradient) 521 -32.6004223337743\n",
      "sum(gradient) 522 -32.61097939336202\n",
      "sum(gradient) 523 -32.621516133596074\n",
      "sum(gradient) 524 -32.63203263248398\n",
      "sum(gradient) 525 -32.642528967585356\n",
      "sum(gradient) 526 -32.65300521601716\n",
      "sum(gradient) 527 -32.6634614544547\n",
      "sum(gradient) 528 -32.67389775913476\n",
      "sum(gradient) 529 -32.68431420586335\n",
      "sum(gradient) 530 -32.69471087000897\n",
      "sum(gradient) 531 -32.70508782651805\n",
      "sum(gradient) 532 -32.71544514990984\n",
      "sum(gradient) 533 -32.72578291428178\n",
      "sum(gradient) 534 -32.7361011933164\n",
      "sum(gradient) 535 -32.74640006027768\n",
      "sum(gradient) 536 -32.75667958801826\n",
      "sum(gradient) 537 -32.76693984898421\n",
      "sum(gradient) 538 -32.77718091521268\n",
      "sum(gradient) 539 -32.78740285834038\n",
      "sum(gradient) 540 -32.7976057496038\n",
      "sum(gradient) 541 -32.80778965983879\n",
      "sum(gradient) 542 -32.817954659494546\n",
      "sum(gradient) 543 -32.82810081862367\n",
      "sum(gradient) 544 -32.83822820689327\n",
      "sum(gradient) 545 -32.8483368935839\n",
      "sum(gradient) 546 -32.858426947593706\n",
      "sum(gradient) 547 -32.868498437442106\n",
      "sum(gradient) 548 -32.87855143127039\n",
      "sum(gradient) 549 -32.88858599684827\n",
      "sum(gradient) 550 -32.898602201571386\n",
      "sum(gradient) 551 -32.90860011246876\n",
      "sum(gradient) 552 -32.91857979620101\n",
      "sum(gradient) 553 -32.92854131906983\n",
      "sum(gradient) 554 -32.93848474701042\n",
      "sum(gradient) 555 -32.94841014560361\n",
      "sum(gradient) 556 -32.958317580075104\n",
      "sum(gradient) 557 -32.968207115296885\n",
      "sum(gradient) 558 -32.97807881578942\n",
      "sum(gradient) 559 -32.98793274572724\n",
      "sum(gradient) 560 -32.997768968937045\n",
      "sum(gradient) 561 -33.00758754890704\n",
      "sum(gradient) 562 -33.01738854877946\n",
      "sum(gradient) 563 -33.027172031362326\n",
      "sum(gradient) 564 -33.036938059126726\n",
      "sum(gradient) 565 -33.0466866942105\n",
      "sum(gradient) 566 -33.05641799842239\n",
      "sum(gradient) 567 -33.06613203323824\n",
      "sum(gradient) 568 -33.07582885981307\n",
      "sum(gradient) 569 -33.08550853897451\n",
      "sum(gradient) 570 -33.09517113123029\n",
      "sum(gradient) 571 -33.10481669676705\n",
      "sum(gradient) 572 -33.11444529545653\n",
      "sum(gradient) 573 -33.12405698685324\n",
      "sum(gradient) 574 -33.13365183020127\n",
      "sum(gradient) 575 -33.14322988443198\n",
      "sum(gradient) 576 -33.1527912081713\n",
      "sum(gradient) 577 -33.162335859734824\n",
      "sum(gradient) 578 -33.171863897138074\n",
      "sum(gradient) 579 -33.18137537809363\n",
      "sum(gradient) 580 -33.19087036001118\n",
      "sum(gradient) 581 -33.2003489000067\n",
      "sum(gradient) 582 -33.209811054899184\n",
      "sum(gradient) 583 -33.21925688121328\n",
      "sum(gradient) 584 -33.22868643517943\n",
      "sum(gradient) 585 -33.23809977274498\n",
      "sum(gradient) 586 -33.247496949562844\n",
      "sum(gradient) 587 -33.256878021005015\n",
      "sum(gradient) 588 -33.26624304215682\n",
      "sum(gradient) 589 -33.27559206782132\n",
      "sum(gradient) 590 -33.284925152526846\n",
      "sum(gradient) 591 -33.29424235051542\n",
      "sum(gradient) 592 -33.30354371576044\n",
      "sum(gradient) 593 -33.31282930195509\n",
      "sum(gradient) 594 -33.32209916252615\n",
      "sum(gradient) 595 -33.33135335062413\n",
      "sum(gradient) 596 -33.34059191913526\n",
      "sum(gradient) 597 -33.34981492067566\n",
      "sum(gradient) 598 -33.35902240759682\n",
      "sum(gradient) 599 -33.3682144319872\n",
      "sum(gradient) 600 -33.37739104567681\n",
      "sum(gradient) 601 -33.38655230023013\n",
      "sum(gradient) 602 -33.39569824695835\n",
      "sum(gradient) 603 -33.40482893691218\n",
      "sum(gradient) 604 -33.41394442089112\n",
      "sum(gradient) 605 -33.4230447494391\n",
      "sum(gradient) 606 -33.43212997284803\n",
      "sum(gradient) 607 -33.44120014116702\n",
      "sum(gradient) 608 -33.450255304185234\n",
      "sum(gradient) 609 -33.459295511457725\n",
      "sum(gradient) 610 -33.46832081228419\n",
      "sum(gradient) 611 -33.477331255731904\n",
      "sum(gradient) 612 -33.486326890615956\n",
      "sum(gradient) 613 -33.49530776551916\n",
      "sum(gradient) 614 -33.504273928779554\n",
      "sum(gradient) 615 -33.51322542850528\n",
      "sum(gradient) 616 -33.52216231256413\n",
      "sum(gradient) 617 -33.53108462859086\n",
      "sum(gradient) 618 -33.53999242398883\n",
      "sum(gradient) 619 -33.5488857459327\n",
      "sum(gradient) 620 -33.557764641361935\n",
      "sum(gradient) 621 -33.56662915699464\n",
      "sum(gradient) 622 -33.5754793393188\n",
      "sum(gradient) 623 -33.58431523459865\n",
      "sum(gradient) 624 -33.593136888874405\n",
      "sum(gradient) 625 -33.601944347963965\n",
      "sum(gradient) 626 -33.61073765746811\n",
      "sum(gradient) 627 -33.61951686276427\n",
      "sum(gradient) 628 -33.628282009011855\n",
      "sum(gradient) 629 -33.63703314115932\n",
      "sum(gradient) 630 -33.645770303934526\n",
      "sum(gradient) 631 -33.654493541853164\n",
      "sum(gradient) 632 -33.66320289922024\n",
      "sum(gradient) 633 -33.67189842012876\n",
      "sum(gradient) 634 -33.680580148462916\n",
      "sum(gradient) 635 -33.68924812789489\n",
      "sum(gradient) 636 -33.69790240189691\n",
      "sum(gradient) 637 -33.7065430137297\n",
      "sum(gradient) 638 -33.71517000645182\n",
      "sum(gradient) 639 -33.72378342291843\n",
      "sum(gradient) 640 -33.73238330578269\n",
      "sum(gradient) 641 -33.74096969749765\n",
      "sum(gradient) 642 -33.74954264031828\n",
      "sum(gradient) 643 -33.75810217629893\n",
      "sum(gradient) 644 -33.76664834729898\n",
      "sum(gradient) 645 -33.775181194982395\n",
      "sum(gradient) 646 -33.783700760817005\n",
      "sum(gradient) 647 -33.79220708608045\n",
      "sum(gradient) 648 -33.800700211852345\n",
      "sum(gradient) 649 -33.80918017903181\n",
      "sum(gradient) 650 -33.81764702831821\n",
      "sum(gradient) 651 -33.82610080022726\n",
      "sum(gradient) 652 -33.8345415350904\n",
      "sum(gradient) 653 -33.84296927304297\n",
      "sum(gradient) 654 -33.85138405404741\n",
      "sum(gradient) 655 -33.859785917874575\n",
      "sum(gradient) 656 -33.86817490411515\n",
      "sum(gradient) 657 -33.87655105217775\n",
      "sum(gradient) 658 -33.88491440129034\n",
      "sum(gradient) 659 -33.89326499050153\n",
      "sum(gradient) 660 -33.90160285868399\n",
      "sum(gradient) 661 -33.90992804452809\n",
      "sum(gradient) 662 -33.91824058655481\n",
      "sum(gradient) 663 -33.926540523102794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum(gradient) 664 -33.934827892343094\n",
      "sum(gradient) 665 -33.94310273226911\n",
      "sum(gradient) 666 -33.95136508070423\n",
      "sum(gradient) 667 -33.95961497530264\n",
      "sum(gradient) 668 -33.9678524535459\n",
      "sum(gradient) 669 -33.976077552745295\n",
      "sum(gradient) 670 -33.9842903100531\n",
      "sum(gradient) 671 -33.99249076243992\n",
      "sum(gradient) 672 -34.00067894672771\n",
      "sum(gradient) 673 -34.008854899560504\n",
      "sum(gradient) 674 -34.017018657423165\n",
      "sum(gradient) 675 -34.02517025663985\n",
      "sum(gradient) 676 -34.03330973336784\n",
      "sum(gradient) 677 -34.0414371236058\n",
      "sum(gradient) 678 -34.04955246319544\n",
      "sum(gradient) 679 -34.05765578781421\n",
      "sum(gradient) 680 -34.065747132985976\n",
      "sum(gradient) 681 -34.07382653407574\n",
      "sum(gradient) 682 -34.08189402628804\n",
      "sum(gradient) 683 -34.08994964467949\n",
      "sum(gradient) 684 -34.097993424148676\n",
      "sum(gradient) 685 -34.10602539943893\n",
      "sum(gradient) 686 -34.11404560514406\n",
      "sum(gradient) 687 -34.12205407570656\n",
      "sum(gradient) 688 -34.130050845413685\n",
      "sum(gradient) 689 -34.13803594840546\n",
      "sum(gradient) 690 -34.146009418674936\n",
      "sum(gradient) 691 -34.153971290060106\n",
      "sum(gradient) 692 -34.16192159626322\n",
      "sum(gradient) 693 -34.1698603708293\n",
      "sum(gradient) 694 -34.17778764716024\n",
      "sum(gradient) 695 -34.18570345851819\n",
      "sum(gradient) 696 -34.19360783801646\n",
      "sum(gradient) 697 -34.20150081862275\n",
      "sum(gradient) 698 -34.20938243317059\n",
      "sum(gradient) 699 -34.21725271434425\n",
      "sum(gradient) 700 -34.22511169469202\n",
      "sum(gradient) 701 -34.23295940662179\n",
      "sum(gradient) 702 -34.240795882397315\n",
      "sum(gradient) 703 -34.24862115415012\n",
      "sum(gradient) 704 -34.256435253875935\n",
      "sum(gradient) 705 -34.264238213423575\n",
      "sum(gradient) 706 -34.27203006451582\n",
      "sum(gradient) 707 -34.27981083873605\n",
      "sum(gradient) 708 -34.2875805675331\n",
      "sum(gradient) 709 -34.29533928222512\n",
      "sum(gradient) 710 -34.303087013992766\n",
      "sum(gradient) 711 -34.31082379388751\n",
      "sum(gradient) 712 -34.31854965283185\n",
      "sum(gradient) 713 -34.326264621609404\n",
      "sum(gradient) 714 -34.33396873088125\n",
      "sum(gradient) 715 -34.34166201118046\n",
      "sum(gradient) 716 -34.34934449290486\n",
      "sum(gradient) 717 -34.3570162063271\n",
      "sum(gradient) 718 -34.36467718159771\n",
      "sum(gradient) 719 -34.3723274487319\n",
      "sum(gradient) 720 -34.37996703762733\n",
      "sum(gradient) 721 -34.38759597805273\n",
      "sum(gradient) 722 -34.39521429965345\n",
      "sum(gradient) 723 -34.402822031951295\n",
      "sum(gradient) 724 -34.41041920434656\n",
      "sum(gradient) 725 -34.41800584611456\n",
      "sum(gradient) 726 -34.42558198641078\n",
      "sum(gradient) 727 -34.43314765426862\n",
      "sum(gradient) 728 -34.44070287860552\n",
      "sum(gradient) 729 -34.44824768821324\n",
      "sum(gradient) 730 -34.45578211176912\n",
      "sum(gradient) 731 -34.463306177830496\n",
      "sum(gradient) 732 -34.4708199148363\n",
      "sum(gradient) 733 -34.478323351113886\n",
      "sum(gradient) 734 -34.48581651486593\n",
      "sum(gradient) 735 -34.49329943418506\n",
      "sum(gradient) 736 -34.50077213704878\n",
      "sum(gradient) 737 -34.50823465131842\n",
      "sum(gradient) 738 -34.515687004742304\n",
      "sum(gradient) 739 -34.52312922495698\n",
      "sum(gradient) 740 -34.53056133948216\n",
      "sum(gradient) 741 -34.537983375730235\n",
      "sum(gradient) 742 -34.54539536100122\n",
      "sum(gradient) 743 -34.55279732248377\n",
      "sum(gradient) 744 -34.56018928725554\n",
      "sum(gradient) 745 -34.567571282286686\n",
      "sum(gradient) 746 -34.574943334435375\n",
      "sum(gradient) 747 -34.58230547045776\n",
      "sum(gradient) 748 -34.589657716994196\n",
      "sum(gradient) 749 -34.59700010058154\n",
      "sum(gradient) 750 -34.60433264765274\n",
      "sum(gradient) 751 -34.61165538453124\n",
      "sum(gradient) 752 -34.61896833743394\n",
      "sum(gradient) 753 -34.626271532477084\n",
      "sum(gradient) 754 -34.63356499566999\n",
      "sum(gradient) 755 -34.640848752917684\n",
      "sum(gradient) 756 -34.6481228300247\n",
      "sum(gradient) 757 -34.65538725268684\n",
      "sum(gradient) 758 -34.66264204650631\n",
      "sum(gradient) 759 -34.66988723697639\n",
      "sum(gradient) 760 -34.67712284949317\n",
      "sum(gradient) 761 -34.68434890935132\n",
      "sum(gradient) 762 -34.69156544174393\n",
      "sum(gradient) 763 -34.698772471769466\n",
      "sum(gradient) 764 -34.70597002442108\n",
      "sum(gradient) 765 -34.7131581245965\n",
      "sum(gradient) 766 -34.72033679709592\n",
      "sum(gradient) 767 -34.72750606662502\n",
      "sum(gradient) 768 -34.734665957782866\n",
      "sum(gradient) 769 -34.74181649508295\n",
      "sum(gradient) 770 -34.74895770293753\n",
      "sum(gradient) 771 -34.756089605660925\n",
      "sum(gradient) 772 -34.763212227482036\n",
      "sum(gradient) 773 -34.77032559252705\n",
      "sum(gradient) 774 -34.77742972482741\n",
      "sum(gradient) 775 -34.78452464832857\n",
      "sum(gradient) 776 -34.79161038687323\n",
      "sum(gradient) 777 -34.798686964222966\n",
      "sum(gradient) 778 -34.80575440403904\n",
      "sum(gradient) 779 -34.81281272989314\n",
      "sum(gradient) 780 -34.81986196526628\n",
      "sum(gradient) 781 -34.82690213354907\n",
      "sum(gradient) 782 -34.833933258043054\n",
      "sum(gradient) 783 -34.840955361959246\n",
      "sum(gradient) 784 -34.84796846841593\n",
      "sum(gradient) 785 -34.854972600451596\n",
      "sum(gradient) 786 -34.86196778100424\n",
      "sum(gradient) 787 -34.868954032936365\n",
      "sum(gradient) 788 -34.875931379014396\n",
      "sum(gradient) 789 -34.88289984192092\n",
      "sum(gradient) 790 -34.88985944425184\n",
      "sum(gradient) 791 -34.89681020851822\n",
      "sum(gradient) 792 -34.903752157143394\n",
      "sum(gradient) 793 -34.91068531246651\n",
      "sum(gradient) 794 -34.91760969674209\n",
      "sum(gradient) 795 -34.924525332142174\n",
      "sum(gradient) 796 -34.93143224074601\n",
      "sum(gradient) 797 -34.93833044456311\n",
      "sum(gradient) 798 -34.94521996551065\n",
      "sum(gradient) 799 -34.95210082542604\n",
      "sum(gradient) 800 -34.95897304606323\n",
      "sum(gradient) 801 -34.96583664909577\n",
      "sum(gradient) 802 -34.972691656114996\n",
      "sum(gradient) 803 -34.979538088630136\n",
      "sum(gradient) 804 -34.98637596807192\n",
      "sum(gradient) 805 -34.99320531579102\n",
      "sum(gradient) 806 -35.000026153056254\n",
      "sum(gradient) 807 -35.00683850105888\n",
      "sum(gradient) 808 -35.013642380906596\n",
      "sum(gradient) 809 -35.02043781363728\n",
      "sum(gradient) 810 -35.02722482020186\n",
      "sum(gradient) 811 -35.034003421480975\n",
      "sum(gradient) 812 -35.04077363827129\n",
      "sum(gradient) 813 -35.047535491294134\n",
      "sum(gradient) 814 -35.054289001197176\n",
      "sum(gradient) 815 -35.06103418854946\n",
      "sum(gradient) 816 -35.06777107384399\n",
      "sum(gradient) 817 -35.07449967749736\n",
      "sum(gradient) 818 -35.08122001985352\n",
      "sum(gradient) 819 -35.08793212117864\n",
      "sum(gradient) 820 -35.09463600167008\n",
      "sum(gradient) 821 -35.101331681442254\n",
      "sum(gradient) 822 -35.10801918054212\n",
      "sum(gradient) 823 -35.11469851894329\n",
      "sum(gradient) 824 -35.12136971654384\n",
      "sum(gradient) 825 -35.12803279317031\n",
      "sum(gradient) 826 -35.13468776857585\n",
      "sum(gradient) 827 -35.14133466244422\n",
      "sum(gradient) 828 -35.14797349438517\n",
      "sum(gradient) 829 -35.15460428393744\n",
      "sum(gradient) 830 -35.16122705057055\n",
      "sum(gradient) 831 -35.167841813683864\n",
      "sum(gradient) 832 -35.17444859259948\n",
      "sum(gradient) 833 -35.18104740658054\n",
      "sum(gradient) 834 -35.18763827481233\n",
      "sum(gradient) 835 -35.19422121641562\n",
      "sum(gradient) 836 -35.20079625043767\n",
      "sum(gradient) 837 -35.20736339586547\n",
      "sum(gradient) 838 -35.2139226716074\n",
      "sum(gradient) 839 -35.220474096507225\n",
      "sum(gradient) 840 -35.22701768934693\n",
      "sum(gradient) 841 -35.233553468837144\n",
      "sum(gradient) 842 -35.24008145361783\n",
      "sum(gradient) 843 -35.24660166227145\n",
      "sum(gradient) 844 -35.25311411330145\n",
      "sum(gradient) 845 -35.25961882515677\n",
      "sum(gradient) 846 -35.26611581621772\n",
      "sum(gradient) 847 -35.272605104795666\n",
      "sum(gradient) 848 -35.27908670914025\n",
      "sum(gradient) 849 -35.28556064743324\n",
      "sum(gradient) 850 -35.29202693779675\n",
      "sum(gradient) 851 -35.29848559828278\n",
      "sum(gradient) 852 -35.304936646887114\n",
      "sum(gradient) 853 -35.31138010153547\n",
      "sum(gradient) 854 -35.31781598009168\n",
      "sum(gradient) 855 -35.32424430035728\n",
      "sum(gradient) 856 -35.33066508007355\n",
      "sum(gradient) 857 -35.33707833691602\n",
      "sum(gradient) 858 -35.34348408849826\n",
      "sum(gradient) 859 -35.34988235237414\n",
      "sum(gradient) 860 -35.356273146037815\n",
      "sum(gradient) 861 -35.36265648691561\n",
      "sum(gradient) 862 -35.36903239237754\n",
      "sum(gradient) 863 -35.375400879732744\n",
      "sum(gradient) 864 -35.38176196623121\n",
      "sum(gradient) 865 -35.38811566905863\n",
      "sum(gradient) 866 -35.39446200534397\n",
      "sum(gradient) 867 -35.40080099215936\n",
      "sum(gradient) 868 -35.407132646510775\n",
      "sum(gradient) 869 -35.413456985351175\n",
      "sum(gradient) 870 -35.41977402557262\n",
      "sum(gradient) 871 -35.426083784006906\n",
      "sum(gradient) 872 -35.432386277433146\n",
      "sum(gradient) 873 -35.43868152256516\n",
      "sum(gradient) 874 -35.444969536067056\n",
      "sum(gradient) 875 -35.451250334536475\n",
      "sum(gradient) 876 -35.45752393452539\n",
      "sum(gradient) 877 -35.46379035251613\n",
      "sum(gradient) 878 -35.47004960494639\n",
      "sum(gradient) 879 -35.476301708188856\n",
      "sum(gradient) 880 -35.4825466785642\n",
      "sum(gradient) 881 -35.48878453233685\n",
      "sum(gradient) 882 -35.49501528571549\n",
      "sum(gradient) 883 -35.50123895485459\n",
      "sum(gradient) 884 -35.50745555584917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum(gradient) 885 -35.51366510474545\n",
      "sum(gradient) 886 -35.5198676175299\n",
      "sum(gradient) 887 -35.52606311014103\n",
      "sum(gradient) 888 -35.53225159845564\n",
      "sum(gradient) 889 -35.538433098303635\n",
      "sum(gradient) 890 -35.544607625455676\n",
      "sum(gradient) 891 -35.55077519563248\n",
      "sum(gradient) 892 -35.5569358245013\n",
      "sum(gradient) 893 -35.56308952767396\n",
      "sum(gradient) 894 -35.5692363207165\n",
      "sum(gradient) 895 -35.57537621913443\n",
      "sum(gradient) 896 -35.581509238384534\n",
      "sum(gradient) 897 -35.58763539387366\n",
      "sum(gradient) 898 -35.59375470095223\n",
      "sum(gradient) 899 -35.59986717492427\n",
      "sum(gradient) 900 -35.60597283104193\n",
      "sum(gradient) 901 -35.612071684503356\n",
      "sum(gradient) 902 -35.61816375045657\n",
      "sum(gradient) 903 -35.62424904399819\n",
      "sum(gradient) 904 -35.63032758018239\n",
      "sum(gradient) 905 -35.63639937400201\n",
      "sum(gradient) 906 -35.64246444040695\n",
      "sum(gradient) 907 -35.648522794295275\n",
      "sum(gradient) 908 -35.65457445051841\n",
      "sum(gradient) 909 -35.660619423872014\n",
      "sum(gradient) 910 -35.66665772910979\n",
      "sum(gradient) 911 -35.67268938093483\n",
      "sum(gradient) 912 -35.678714393997694\n",
      "sum(gradient) 913 -35.684732782905535\n",
      "sum(gradient) 914 -35.69074456221588\n",
      "sum(gradient) 915 -35.696749746437774\n",
      "sum(gradient) 916 -35.70274835003114\n",
      "sum(gradient) 917 -35.708740387409804\n",
      "sum(gradient) 918 -35.7147258729456\n",
      "sum(gradient) 919 -35.72070482095258\n",
      "sum(gradient) 920 -35.72667724570571\n",
      "sum(gradient) 921 -35.73264316143081\n",
      "sum(gradient) 922 -35.738602582306854\n",
      "sum(gradient) 923 -35.74455552246997\n",
      "sum(gradient) 924 -35.7505019960035\n",
      "sum(gradient) 925 -35.75644201695172\n",
      "sum(gradient) 926 -35.76237559930725\n",
      "sum(gradient) 927 -35.768302757024586\n",
      "sum(gradient) 928 -35.774223504008695\n",
      "sum(gradient) 929 -35.78013785411535\n",
      "sum(gradient) 930 -35.786045821161395\n",
      "sum(gradient) 931 -35.79194741891974\n",
      "sum(gradient) 932 -35.797842661113926\n",
      "sum(gradient) 933 -35.80373156142504\n",
      "sum(gradient) 934 -35.80961413349029\n",
      "sum(gradient) 935 -35.81549039090538\n",
      "sum(gradient) 936 -35.821360347220704\n",
      "sum(gradient) 937 -35.82722401593831\n",
      "sum(gradient) 938 -35.833081410524365\n",
      "sum(gradient) 939 -35.83893254439892\n",
      "sum(gradient) 940 -35.84477743093663\n",
      "sum(gradient) 941 -35.85061608347324\n",
      "sum(gradient) 942 -35.85644851529835\n",
      "sum(gradient) 943 -35.86227473966066\n",
      "sum(gradient) 944 -35.86809476977002\n",
      "sum(gradient) 945 -35.87390861878531\n",
      "sum(gradient) 946 -35.879716299835685\n",
      "sum(gradient) 947 -35.885517825998576\n",
      "sum(gradient) 948 -35.89131321031093\n",
      "sum(gradient) 949 -35.897102465773685\n",
      "sum(gradient) 950 -35.90288560534376\n",
      "sum(gradient) 951 -35.908662641934505\n",
      "sum(gradient) 952 -35.9144335884237\n",
      "sum(gradient) 953 -35.920198457644794\n",
      "sum(gradient) 954 -35.92595726238887\n",
      "sum(gradient) 955 -35.93171001541001\n",
      "sum(gradient) 956 -35.937456729423666\n",
      "sum(gradient) 957 -35.94319741710156\n",
      "sum(gradient) 958 -35.948932091077495\n",
      "sum(gradient) 959 -35.954660763942286\n",
      "sum(gradient) 960 -35.96038344825237\n",
      "sum(gradient) 961 -35.9661001565239\n",
      "sum(gradient) 962 -35.97181090123038\n",
      "sum(gradient) 963 -35.97751569480892\n",
      "sum(gradient) 964 -35.98321454965554\n",
      "sum(gradient) 965 -35.98890747813171\n",
      "sum(gradient) 966 -35.994594492557695\n",
      "sum(gradient) 967 -36.000275605213886\n",
      "sum(gradient) 968 -36.00595082834577\n",
      "sum(gradient) 969 -36.011620174156455\n",
      "sum(gradient) 970 -36.01728365481887\n",
      "sum(gradient) 971 -36.022941282459456\n",
      "sum(gradient) 972 -36.02859306917133\n",
      "sum(gradient) 973 -36.03423902701245\n",
      "sum(gradient) 974 -36.03987916799403\n",
      "sum(gradient) 975 -36.045513504105514\n",
      "sum(gradient) 976 -36.05114204728442\n",
      "sum(gradient) 977 -36.05676480944164\n",
      "sum(gradient) 978 -36.06238180244499\n",
      "sum(gradient) 979 -36.06799303812814\n",
      "sum(gradient) 980 -36.07359852829133\n",
      "sum(gradient) 981 -36.07919828469313\n",
      "sum(gradient) 982 -36.08479231906019\n",
      "sum(gradient) 983 -36.09038064308115\n",
      "sum(gradient) 984 -36.095963268410834\n",
      "sum(gradient) 985 -36.10154020666526\n",
      "sum(gradient) 986 -36.10711146942768\n",
      "sum(gradient) 987 -36.11267706824821\n",
      "sum(gradient) 988 -36.118237014633486\n",
      "sum(gradient) 989 -36.123791320062935\n",
      "sum(gradient) 990 -36.12933999597827\n",
      "sum(gradient) 991 -36.13488305378546\n",
      "sum(gradient) 992 -36.140420504859996\n",
      "sum(gradient) 993 -36.145952360537976\n",
      "sum(gradient) 994 -36.15147863212276\n",
      "sum(gradient) 995 -36.156999330885185\n",
      "sum(gradient) 996 -36.162514468059015\n",
      "sum(gradient) 997 -36.16802405484713\n",
      "sum(gradient) 998 -36.173528102416505\n",
      "sum(gradient) 999 -36.17902662190111\n",
      "sum(gradient) 1000 -36.184519624403244\n",
      "sum(gradient) 1001 -36.19000712098776\n",
      "sum(gradient) 1002 -36.195489122689175\n",
      "sum(gradient) 1003 -36.200965640507846\n",
      "sum(gradient) 1004 -36.20643668541271\n",
      "sum(gradient) 1005 -36.211902268337326\n",
      "sum(gradient) 1006 -36.217362400184726\n",
      "sum(gradient) 1007 -36.222817091826066\n",
      "sum(gradient) 1008 -36.22826635409408\n",
      "sum(gradient) 1009 -36.23371019779691\n",
      "sum(gradient) 1010 -36.23914863370485\n",
      "sum(gradient) 1011 -36.24458167256073\n",
      "sum(gradient) 1012 -36.25000932507159\n",
      "sum(gradient) 1013 -36.2554316019153\n",
      "sum(gradient) 1014 -36.2608485137344\n",
      "sum(gradient) 1015 -36.26626007114411\n",
      "sum(gradient) 1016 -36.271666284727715\n",
      "sum(gradient) 1017 -36.27706716503218\n",
      "sum(gradient) 1018 -36.28246272257895\n",
      "sum(gradient) 1019 -36.28785296785892\n",
      "sum(gradient) 1020 -36.293237911324496\n",
      "sum(gradient) 1021 -36.29861756340175\n",
      "sum(gradient) 1022 -36.30399193449346\n",
      "sum(gradient) 1023 -36.309361034958606\n",
      "sum(gradient) 1024 -36.31472487513385\n",
      "sum(gradient) 1025 -36.320083465322995\n",
      "sum(gradient) 1026 -36.3254368158\n",
      "sum(gradient) 1027 -36.33078493681072\n",
      "sum(gradient) 1028 -36.336127838566114\n",
      "sum(gradient) 1029 -36.341465531253874\n",
      "sum(gradient) 1030 -36.346798025024135\n",
      "sum(gradient) 1031 -36.35212533000474\n",
      "sum(gradient) 1032 -36.35744745629069\n",
      "sum(gradient) 1033 -36.362764413943594\n",
      "sum(gradient) 1034 -36.36807621300626\n",
      "sum(gradient) 1035 -36.373382863479065\n",
      "sum(gradient) 1036 -36.37868437534803\n",
      "sum(gradient) 1037 -36.38398075855597\n",
      "sum(gradient) 1038 -36.38927202302402\n",
      "sum(gradient) 1039 -36.3945581786452\n",
      "sum(gradient) 1040 -36.3998392352825\n",
      "sum(gradient) 1041 -36.40511520276916\n",
      "sum(gradient) 1042 -36.41038609091359\n",
      "sum(gradient) 1043 -36.415651909488325\n",
      "sum(gradient) 1044 -36.420912668246636\n",
      "sum(gradient) 1045 -36.426168376910276\n",
      "sum(gradient) 1046 -36.431419045172376\n",
      "sum(gradient) 1047 -36.436664682696204\n",
      "sum(gradient) 1048 -36.44190529912073\n",
      "sum(gradient) 1049 -36.447140904055864\n",
      "sum(gradient) 1050 -36.452371507083576\n",
      "sum(gradient) 1051 -36.457597117760834\n",
      "sum(gradient) 1052 -36.46281774561425\n",
      "sum(gradient) 1053 -36.46803340014491\n",
      "sum(gradient) 1054 -36.47324409082278\n",
      "sum(gradient) 1055 -36.47844982709789\n",
      "sum(gradient) 1056 -36.483650618388076\n",
      "sum(gradient) 1057 -36.488846474087154\n",
      "sum(gradient) 1058 -36.49403740355836\n",
      "sum(gradient) 1059 -36.49922341614456\n",
      "sum(gradient) 1060 -36.50440452115416\n",
      "sum(gradient) 1061 -36.509580727876354\n",
      "sum(gradient) 1062 -36.51475204557023\n",
      "sum(gradient) 1063 -36.51991848347023\n",
      "sum(gradient) 1064 -36.525080050780375\n",
      "sum(gradient) 1065 -36.53023675668734\n",
      "sum(gradient) 1066 -36.53538861034115\n",
      "sum(gradient) 1067 -36.54053562087665\n",
      "sum(gradient) 1068 -36.545677797392884\n",
      "sum(gradient) 1069 -36.55081514897243\n",
      "sum(gradient) 1070 -36.55594768466805\n",
      "sum(gradient) 1071 -36.561075413505826\n",
      "sum(gradient) 1072 -36.566198344486516\n",
      "sum(gradient) 1073 -36.571316486586305\n",
      "sum(gradient) 1074 -36.576429848759744\n",
      "sum(gradient) 1075 -36.58153843993151\n",
      "sum(gradient) 1076 -36.586642269004386\n",
      "sum(gradient) 1077 -36.59174134485282\n",
      "sum(gradient) 1078 -36.59683567633182\n",
      "sum(gradient) 1079 -36.60192527226465\n",
      "sum(gradient) 1080 -36.60701014145621\n",
      "sum(gradient) 1081 -36.61209029268573\n",
      "sum(gradient) 1082 -36.617165734704024\n",
      "sum(gradient) 1083 -36.62223647624053\n",
      "sum(gradient) 1084 -36.62730252600502\n",
      "sum(gradient) 1085 -36.632363892672096\n",
      "sum(gradient) 1086 -36.637420584902806\n",
      "sum(gradient) 1087 -36.64247261132448\n",
      "sum(gradient) 1088 -36.64751998055309\n",
      "sum(gradient) 1089 -36.652562701170545\n",
      "sum(gradient) 1090 -36.65760078173802\n",
      "sum(gradient) 1091 -36.662634230793515\n",
      "sum(gradient) 1092 -36.66766305685235\n",
      "sum(gradient) 1093 -36.67268726840286\n",
      "sum(gradient) 1094 -36.677706873915035\n",
      "sum(gradient) 1095 -36.68272188183082\n",
      "sum(gradient) 1096 -36.68773230057534\n",
      "sum(gradient) 1097 -36.69273813853948\n",
      "sum(gradient) 1098 -36.69773940410482\n",
      "sum(gradient) 1099 -36.70273610561862\n",
      "sum(gradient) 1100 -36.707728251413045\n",
      "sum(gradient) 1101 -36.71271584979243\n",
      "sum(gradient) 1102 -36.71769890904236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum(gradient) 1103 -36.72267743742113\n",
      "sum(gradient) 1104 -36.72765144316806\n",
      "sum(gradient) 1105 -36.732620934501945\n",
      "sum(gradient) 1106 -36.73758591961253\n",
      "sum(gradient) 1107 -36.74254640667231\n",
      "sum(gradient) 1108 -36.74750240383027\n",
      "sum(gradient) 1109 -36.752453919215604\n",
      "sum(gradient) 1110 -36.7574009609305\n",
      "sum(gradient) 1111 -36.76234353705937\n",
      "sum(gradient) 1112 -36.767281655663105\n",
      "sum(gradient) 1113 -36.77221532478137\n",
      "sum(gradient) 1114 -36.777144552428545\n",
      "sum(gradient) 1115 -36.78206934660559\n",
      "sum(gradient) 1116 -36.786989715285145\n",
      "sum(gradient) 1117 -36.79190566641835\n",
      "sum(gradient) 1118 -36.796817207939235\n",
      "sum(gradient) 1119 -36.80172434775647\n",
      "sum(gradient) 1120 -36.80662709375649\n",
      "sum(gradient) 1121 -36.81152545381138\n",
      "sum(gradient) 1122 -36.816419435766235\n",
      "sum(gradient) 1123 -36.82130904744472\n",
      "sum(gradient) 1124 -36.826194296655906\n",
      "sum(gradient) 1125 -36.831075191179494\n",
      "sum(gradient) 1126 -36.83595173877773\n",
      "sum(gradient) 1127 -36.84082394719411\n",
      "sum(gradient) 1128 -36.845691824152404\n",
      "sum(gradient) 1129 -36.85055537735044\n",
      "sum(gradient) 1130 -36.85541461446934\n",
      "sum(gradient) 1131 -36.860269543170844\n",
      "sum(gradient) 1132 -36.865120171092045\n",
      "sum(gradient) 1133 -36.869966505855174\n",
      "sum(gradient) 1134 -36.87480855505319\n",
      "sum(gradient) 1135 -36.87964632627361\n",
      "sum(gradient) 1136 -36.88447982706784\n",
      "sum(gradient) 1137 -36.88930906497849\n",
      "sum(gradient) 1138 -36.894134047522954\n",
      "sum(gradient) 1139 -36.89895478220108\n",
      "sum(gradient) 1140 -36.90377127649447\n",
      "sum(gradient) 1141 -36.90858353785529\n",
      "sum(gradient) 1142 -36.91339157373193\n",
      "sum(gradient) 1143 -36.918195391537544\n",
      "sum(gradient) 1144 -36.922994998675144\n",
      "sum(gradient) 1145 -36.92779040253071\n",
      "sum(gradient) 1146 -36.93258161045809\n",
      "sum(gradient) 1147 -36.9373686298043\n",
      "sum(gradient) 1148 -36.942151467894\n",
      "sum(gradient) 1149 -36.946930132026594\n",
      "sum(gradient) 1150 -36.951704629490415\n",
      "sum(gradient) 1151 -36.956474967551486\n",
      "sum(gradient) 1152 -36.96124115345509\n",
      "sum(gradient) 1153 -36.96600319442856\n",
      "sum(gradient) 1154 -36.97076109768414\n",
      "sum(gradient) 1155 -36.97551487040861\n",
      "sum(gradient) 1156 -36.980264519775226\n",
      "sum(gradient) 1157 -36.98501005293803\n",
      "sum(gradient) 1158 -36.98975147702946\n",
      "sum(gradient) 1159 -36.99448879916712\n",
      "sum(gradient) 1160 -36.999222026447924\n",
      "sum(gradient) 1161 -37.00395116594864\n",
      "sum(gradient) 1162 -37.00867622473238\n",
      "sum(gradient) 1163 -37.013397209840356\n",
      "sum(gradient) 1164 -37.01811412829707\n",
      "sum(gradient) 1165 -37.02282698710875\n",
      "sum(gradient) 1166 -37.0275357932622\n",
      "sum(gradient) 1167 -37.03224055372869\n",
      "sum(gradient) 1168 -37.03694127546106\n",
      "sum(gradient) 1169 -37.041637965389675\n",
      "sum(gradient) 1170 -37.04633063043184\n",
      "sum(gradient) 1171 -37.05101927748997\n",
      "sum(gradient) 1172 -37.05570391343908\n",
      "sum(gradient) 1173 -37.06038454514653\n",
      "sum(gradient) 1174 -37.065061179454474\n",
      "sum(gradient) 1175 -37.06973382319533\n",
      "sum(gradient) 1176 -37.07440248317482\n",
      "sum(gradient) 1177 -37.07906716618717\n",
      "sum(gradient) 1178 -37.08372787900768\n",
      "sum(gradient) 1179 -37.08838462839584\n",
      "sum(gradient) 1180 -37.093037421092845\n",
      "sum(gradient) 1181 -37.097686263822446\n",
      "sum(gradient) 1182 -37.10233116329015\n",
      "sum(gradient) 1183 -37.106972126187145\n",
      "sum(gradient) 1184 -37.11160915918727\n",
      "sum(gradient) 1185 -37.11624226894053\n",
      "sum(gradient) 1186 -37.120871462091486\n",
      "sum(gradient) 1187 -37.12549674526278\n",
      "sum(gradient) 1188 -37.13011812505589\n",
      "sum(gradient) 1189 -37.13473560806211\n",
      "sum(gradient) 1190 -37.13934920084899\n",
      "sum(gradient) 1191 -37.1439589099788\n",
      "sum(gradient) 1192 -37.14856474198335\n",
      "sum(gradient) 1193 -37.15316670338794\n",
      "sum(gradient) 1194 -37.15776480070049\n",
      "sum(gradient) 1195 -37.16235904040536\n",
      "sum(gradient) 1196 -37.166949428980146\n",
      "sum(gradient) 1197 -37.17153597288016\n",
      "sum(gradient) 1198 -37.17611867854479\n",
      "sum(gradient) 1199 -37.18069755239847\n",
      "sum(gradient) 1200 -37.18527260085269\n",
      "sum(gradient) 1201 -37.1898438302961\n",
      "sum(gradient) 1202 -37.19441124710502\n",
      "sum(gradient) 1203 -37.198974857642305\n",
      "sum(gradient) 1204 -37.203534668249866\n",
      "sum(gradient) 1205 -37.20809068525552\n",
      "sum(gradient) 1206 -37.212642914974616\n",
      "sum(gradient) 1207 -37.2171913637048\n",
      "sum(gradient) 1208 -37.22173603772243\n",
      "sum(gradient) 1209 -37.226276943299716\n",
      "sum(gradient) 1210 -37.23081408667897\n",
      "sum(gradient) 1211 -37.235347474102305\n",
      "sum(gradient) 1212 -37.23987711178421\n",
      "sum(gradient) 1213 -37.244403005927765\n",
      "sum(gradient) 1214 -37.2489251627253\n",
      "sum(gradient) 1215 -37.25344358834526\n",
      "sum(gradient) 1216 -37.25795828894839\n",
      "sum(gradient) 1217 -37.262469270676334\n",
      "sum(gradient) 1218 -37.2669765396578\n",
      "sum(gradient) 1219 -37.271480101998314\n",
      "sum(gradient) 1220 -37.27597996380035\n",
      "sum(gradient) 1221 -37.280476131146415\n",
      "sum(gradient) 1222 -37.2849686101032\n",
      "sum(gradient) 1223 -37.2894574067199\n",
      "sum(gradient) 1224 -37.29394252703571\n",
      "sum(gradient) 1225 -37.2984239770721\n",
      "sum(gradient) 1226 -37.302901762838104\n",
      "sum(gradient) 1227 -37.30737589032499\n",
      "sum(gradient) 1228 -37.31184636551194\n",
      "sum(gradient) 1229 -37.31631319436068\n",
      "sum(gradient) 1230 -37.320776382823325\n",
      "sum(gradient) 1231 -37.32523593683123\n",
      "sum(gradient) 1232 -37.32969186230517\n",
      "sum(gradient) 1233 -37.33414416515194\n",
      "sum(gradient) 1234 -37.33859285125996\n",
      "sum(gradient) 1235 -37.343037926506184\n",
      "sum(gradient) 1236 -37.34747939675537\n",
      "sum(gradient) 1237 -37.35191726785458\n",
      "sum(gradient) 1238 -37.3563515456371\n",
      "sum(gradient) 1239 -37.360782235921235\n",
      "sum(gradient) 1240 -37.36520934451333\n",
      "sum(gradient) 1241 -37.36963287720857\n",
      "sum(gradient) 1242 -37.37405283977927\n",
      "sum(gradient) 1243 -37.37846923799115\n",
      "sum(gradient) 1244 -37.382882077595305\n",
      "sum(gradient) 1245 -37.38729136432264\n",
      "sum(gradient) 1246 -37.39169710389948\n",
      "sum(gradient) 1247 -37.39609930203201\n",
      "sum(gradient) 1248 -37.4004979644128\n",
      "sum(gradient) 1249 -37.40489309672595\n",
      "sum(gradient) 1250 -37.40928470463589\n",
      "sum(gradient) 1251 -37.413672793795506\n",
      "sum(gradient) 1252 -37.41805736984441\n",
      "sum(gradient) 1253 -37.42243843841114\n",
      "sum(gradient) 1254 -37.42681600510269\n",
      "sum(gradient) 1255 -37.43119007552434\n",
      "sum(gradient) 1256 -37.43556065525949\n",
      "sum(gradient) 1257 -37.43992774987819\n",
      "sum(gradient) 1258 -37.44429136494174\n",
      "sum(gradient) 1259 -37.44865150599327\n",
      "sum(gradient) 1260 -37.453008178569114\n",
      "sum(gradient) 1261 -37.45736138818516\n",
      "sum(gradient) 1262 -37.46171114034812\n",
      "sum(gradient) 1263 -37.466057440552326\n",
      "sum(gradient) 1264 -37.470400294277134\n",
      "sum(gradient) 1265 -37.47473970698807\n",
      "sum(gradient) 1266 -37.479075684139374\n",
      "sum(gradient) 1267 -37.48340823117066\n",
      "sum(gradient) 1268 -37.48773735351358\n",
      "sum(gradient) 1269 -37.492063056579255\n",
      "sum(gradient) 1270 -37.49638534577307\n",
      "sum(gradient) 1271 -37.50070422648105\n",
      "sum(gradient) 1272 -37.50501970408222\n",
      "sum(gradient) 1273 -37.50933178394062\n",
      "sum(gradient) 1274 -37.51364047140702\n",
      "sum(gradient) 1275 -37.51794577182061\n",
      "sum(gradient) 1276 -37.522247690505246\n",
      "sum(gradient) 1277 -37.52654623277542\n",
      "sum(gradient) 1278 -37.53084140393503\n",
      "sum(gradient) 1279 -37.53513320926762\n",
      "sum(gradient) 1280 -37.53942165405451\n",
      "sum(gradient) 1281 -37.543706743556335\n",
      "sum(gradient) 1282 -37.547988483023296\n",
      "sum(gradient) 1283 -37.5522668776957\n",
      "sum(gradient) 1284 -37.55654193280249\n",
      "sum(gradient) 1285 -37.560813653555705\n",
      "sum(gradient) 1286 -37.56508204515772\n",
      "sum(gradient) 1287 -37.56934711279947\n",
      "sum(gradient) 1288 -37.57360886165736\n",
      "sum(gradient) 1289 -37.57786729690083\n",
      "sum(gradient) 1290 -37.582122423679586\n",
      "sum(gradient) 1291 -37.586374247138686\n",
      "sum(gradient) 1292 -37.59062277240599\n",
      "sum(gradient) 1293 -37.59486800460044\n",
      "sum(gradient) 1294 -37.599109948826445\n",
      "sum(gradient) 1295 -37.60334861018024\n",
      "sum(gradient) 1296 -37.60758399374285\n",
      "sum(gradient) 1297 -37.6118161045865\n",
      "sum(gradient) 1298 -37.616044947768344\n",
      "sum(gradient) 1299 -37.620270528334494\n",
      "sum(gradient) 1300 -37.624492851322636\n",
      "sum(gradient) 1301 -37.62871192175565\n",
      "sum(gradient) 1302 -37.632927744646146\n",
      "sum(gradient) 1303 -37.63714032499317\n",
      "sum(gradient) 1304 -37.64134966778743\n",
      "sum(gradient) 1305 -37.645555778004734\n",
      "sum(gradient) 1306 -37.64975866061245\n",
      "sum(gradient) 1307 -37.653958320564335\n",
      "sum(gradient) 1308 -37.65815476280436\n",
      "sum(gradient) 1309 -37.662347992264955\n",
      "sum(gradient) 1310 -37.66653801386361\n",
      "sum(gradient) 1311 -37.67072483251195\n",
      "sum(gradient) 1312 -37.67490845310833\n",
      "sum(gradient) 1313 -37.67908888053879\n",
      "sum(gradient) 1314 -37.683266119677846\n",
      "sum(gradient) 1315 -37.68744017539089\n",
      "sum(gradient) 1316 -37.69161105252994\n",
      "sum(gradient) 1317 -37.69577875594053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum(gradient) 1318 -37.69994329045011\n",
      "sum(gradient) 1319 -37.70410466088119\n",
      "sum(gradient) 1320 -37.70826287203966\n",
      "sum(gradient) 1321 -37.71241792872675\n",
      "sum(gradient) 1322 -37.71656983572899\n",
      "sum(gradient) 1323 -37.72071859782011\n",
      "sum(gradient) 1324 -37.72486421977168\n",
      "sum(gradient) 1325 -37.72900670633306\n",
      "sum(gradient) 1326 -37.733146062248004\n",
      "sum(gradient) 1327 -37.73728229225248\n",
      "sum(gradient) 1328 -37.74141540106707\n",
      "sum(gradient) 1329 -37.745545393404505\n",
      "sum(gradient) 1330 -37.74967227396334\n",
      "sum(gradient) 1331 -37.75379604743782\n",
      "sum(gradient) 1332 -37.757916718503836\n",
      "sum(gradient) 1333 -37.762034291833885\n",
      "sum(gradient) 1334 -37.76614877208675\n",
      "sum(gradient) 1335 -37.77026016390587\n",
      "sum(gradient) 1336 -37.77436847193358\n",
      "sum(gradient) 1337 -37.77847370079532\n",
      "sum(gradient) 1338 -37.782575855109656\n",
      "sum(gradient) 1339 -37.786674939481244\n",
      "sum(gradient) 1340 -37.79077095850452\n",
      "sum(gradient) 1341 -37.79486391677036\n",
      "sum(gradient) 1342 -37.79895381885111\n",
      "sum(gradient) 1343 -37.803040669311976\n",
      "sum(gradient) 1344 -37.80712447270887\n",
      "sum(gradient) 1345 -37.81120523358596\n",
      "sum(gradient) 1346 -37.81528295648075\n",
      "sum(gradient) 1347 -37.81935764591207\n",
      "sum(gradient) 1348 -37.823429306400676\n",
      "sum(gradient) 1349 -37.827497942447515\n",
      "sum(gradient) 1350 -37.83156355854748\n",
      "sum(gradient) 1351 -37.835626159184805\n",
      "sum(gradient) 1352 -37.83968574883346\n",
      "sum(gradient) 1353 -37.84374233195889\n",
      "sum(gradient) 1354 -37.847795913017585\n",
      "sum(gradient) 1355 -37.85184649645021\n",
      "sum(gradient) 1356 -37.855894086693716\n",
      "sum(gradient) 1357 -37.85993868817312\n",
      "sum(gradient) 1358 -37.86398030530296\n",
      "sum(gradient) 1359 -37.86801894248685\n",
      "sum(gradient) 1360 -37.87205460412443\n",
      "sum(gradient) 1361 -37.87608729459834\n",
      "sum(gradient) 1362 -37.88011701828563\n",
      "sum(gradient) 1363 -37.8841437795518\n",
      "sum(gradient) 1364 -37.888167582756836\n",
      "sum(gradient) 1365 -37.892188432246115\n",
      "sum(gradient) 1366 -37.89620633235728\n",
      "sum(gradient) 1367 -37.900221287418255\n",
      "sum(gradient) 1368 -37.904233301748334\n",
      "sum(gradient) 1369 -37.90824237965533\n",
      "sum(gradient) 1370 -37.91224852544088\n",
      "sum(gradient) 1371 -37.91625174339515\n",
      "sum(gradient) 1372 -37.92025203779629\n",
      "sum(gradient) 1373 -37.924249412920844\n",
      "sum(gradient) 1374 -37.92824387302456\n",
      "sum(gradient) 1375 -37.93223542236331\n",
      "sum(gradient) 1376 -37.93622406518269\n",
      "sum(gradient) 1377 -37.9402098057133\n",
      "sum(gradient) 1378 -37.94419264818182\n",
      "sum(gradient) 1379 -37.948172596806835\n",
      "sum(gradient) 1380 -37.952149655786606\n",
      "sum(gradient) 1381 -37.956123829329776\n",
      "sum(gradient) 1382 -37.96009512161474\n",
      "sum(gradient) 1383 -37.96406353682543\n",
      "sum(gradient) 1384 -37.96802907913171\n",
      "sum(gradient) 1385 -37.9719917526933\n",
      "sum(gradient) 1386 -37.975951561662995\n",
      "sum(gradient) 1387 -37.97990851018267\n",
      "sum(gradient) 1388 -37.98386260238946\n",
      "sum(gradient) 1389 -37.98781384240225\n",
      "sum(gradient) 1390 -37.991762234345366\n",
      "sum(gradient) 1391 -37.99570778231678\n",
      "sum(gradient) 1392 -37.99965049042316\n",
      "sum(gradient) 1393 -38.003590362750735\n",
      "sum(gradient) 1394 -38.00752740337773\n",
      "sum(gradient) 1395 -38.011461616378995\n",
      "sum(gradient) 1396 -38.015393005816755\n",
      "sum(gradient) 1397 -38.01932157574257\n",
      "sum(gradient) 1398 -38.02324733020807\n",
      "sum(gradient) 1399 -38.02717027324255\n",
      "sum(gradient) 1400 -38.0310904088814\n",
      "sum(gradient) 1401 -38.03500774114077\n",
      "sum(gradient) 1402 -38.0389222740314\n",
      "sum(gradient) 1403 -38.042834011555414\n",
      "sum(gradient) 1404 -38.04674295770922\n",
      "sum(gradient) 1405 -38.05064911647265\n",
      "sum(gradient) 1406 -38.054552491828325\n",
      "sum(gradient) 1407 -38.05845308773982\n",
      "sum(gradient) 1408 -38.06235090816901\n",
      "sum(gradient) 1409 -38.06624595706656\n",
      "sum(gradient) 1410 -38.07013823837619\n",
      "sum(gradient) 1411 -38.07402775603178\n",
      "sum(gradient) 1412 -38.07791451395786\n",
      "sum(gradient) 1413 -38.08179851607351\n",
      "sum(gradient) 1414 -38.08567976628905\n",
      "sum(gradient) 1415 -38.089558268502955\n",
      "sum(gradient) 1416 -38.09343402660967\n",
      "sum(gradient) 1417 -38.09730704449259\n",
      "sum(gradient) 1418 -38.10117732602948\n",
      "sum(gradient) 1419 -38.10504487508637\n",
      "sum(gradient) 1420 -38.10890969552273\n",
      "sum(gradient) 1421 -38.112771791192564\n",
      "sum(gradient) 1422 -38.11663116593849\n",
      "sum(gradient) 1423 -38.12048782359425\n",
      "sum(gradient) 1424 -38.12434176798846\n",
      "sum(gradient) 1425 -38.12819300294014\n",
      "sum(gradient) 1426 -38.13204153225876\n",
      "sum(gradient) 1427 -38.13588735975176\n",
      "sum(gradient) 1428 -38.139730489209846\n",
      "sum(gradient) 1429 -38.143570924421724\n",
      "sum(gradient) 1430 -38.147408669164804\n",
      "sum(gradient) 1431 -38.15124372721276\n",
      "sum(gradient) 1432 -38.155076102327016\n",
      "sum(gradient) 1433 -38.15890579826258\n",
      "sum(gradient) 1434 -38.16273281876913\n",
      "sum(gradient) 1435 -38.166557167585424\n",
      "sum(gradient) 1436 -38.1703788484413\n",
      "sum(gradient) 1437 -38.17419786506164\n",
      "sum(gradient) 1438 -38.17801422116254\n",
      "sum(gradient) 1439 -38.18182792045338\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "y_pred,score, theta = train_predict(x_train,y_train,x_val,y_val) \n",
    "#print('Accuracy score',score)\n",
    "#print('y_pred', y_pred)\n",
    "\n",
    "\n",
    "#print(classification_report(y_val,y_pred)) # main classification metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a prediction-only function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(xTest,yTest,theta): # using value of theta learned from training model\n",
    "    xTest = xTest - xTest.mean() # normalise\n",
    "    new_col = np.zeros((xTest.shape[0], 1)) \n",
    "    xTest = np.append(xTest, new_col, axis=1)\n",
    "    z = np.dot(xTest,theta)\n",
    "    h = sigmoid(z)\n",
    "    y_pred = h >= 0.5 # true or false assignment \n",
    "    score = accuracy_score(yTest,y_pred)\n",
    "    \n",
    "    cm = confusion_matrix(yTest,y_pred)\n",
    "    display = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    display.plot()\n",
    "    plt.show()    \n",
    "    return y_pred, score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on unseen test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score2 0.8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.08      0.14        12\n",
      "           1       0.81      0.98      0.89        48\n",
      "\n",
      "    accuracy                           0.80        60\n",
      "   macro avg       0.66      0.53      0.51        60\n",
      "weighted avg       0.75      0.80      0.74        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred2,score2 = predict(x_test,y_test,theta)\n",
    "print('Accuracy score2',score2)\n",
    "print(classification_report(y_test,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
